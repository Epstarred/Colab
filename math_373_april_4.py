# -*- coding: utf-8 -*-
"""Math 373 April 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zz4bbNtky1d0-FhPQrnR1djGB-bPisgn
"""

#Pytorch uses object oriented style
import torch
import numpy as np
import pandas as pd
import sklearn as sk
import sklearn.model_selection

df_labeled = pd.read_csv("./sample_data/mnist_train_small.csv")
df_train, df_val = sk.model_selection.train_test_split(df_labeled)

class DigitsDataset():
  def __init__(self, df): # Method 1, constructor
    self.df = df
  def __len__(self): # Method 2, length
    return len(self.df)
  def __getitem__(self, i): #Method 3, get item - pandas to tensor
    row = self.df.iloc[i] #iloc says think of i as an int instead of an index
    yi = torch.tensor(row[0]) #label
    xi = torch.tensor(row[1:])/255.0 #image
    return xi, yi

train_dataset = DigitsDataset(df_train) #This object grabs this data for us
val_dataset = DigitsDataset(df_val)

i = np.random.randint(len(df_train))
x, y = train_dataset.__getitem__(i)

import matplotlib.pyplot as plt
plt.imshow(torch.reshape(x, (28,28)))
plt.title(str(y))

#Create a data loader object for a batch of data
#At the begining of each epoch it should shuffle the training data to ensure randomness
dataloader_train = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle=True)
dataloader_val = torch.utils.data.DataLoader(val_dataset, batch_size = 16, shuffle=False)

x_batch, y_batch = next(iter(dataloader_train))
print("x_batch.shape is:", x_batch.shape)
print("y_batch.shape is:", y_batch.shape)

plt.imshow(torch.reshape(x_batch[7], (28,28)), cmap ='gray')

class SimpleNeuralNetwork(torch.nn.Module): #A subclass of Module, a generic NN
  def __init__(self): #Constructor is going to specify layers
    super().__init__()
    self.dense1 = torch.nn.Linear(784, 500) #784 is dim of input and #500 is dim of output
    self.dense2 = torch.nn.Linear(500,250)
    self.dense3 = torch.nn.Linear(250,10) #Output of last layer must match # of features

  def forward(self, x): #feeds x forward through the model
    x = self.dense1(x)
    x = torch.nn.functional.relu(x)

    x = self.dense2(x)
    x = torch.nn.functional.relu(x)

    x = self.dense3(x)
    return x



model = SimpleNeuralNetwork() #untrained neural network

x = model.dense1(x_batch)
x = torch.nn.functional.relu(x)
x = model.dense2(x)
x = torch.nn.functional.relu(x)
x = model.dense3(x)

y_pred = model(x_batch) #random predictions

y_pred.shape #shape is correct

#We need a loss function
loss_fun = torch.nn.CrossEntropyLoss()
optim = torch.optim.Adam(lr =.001, params=model.parameters()) #need learning rate and what variables to optimize

num_epochs = 5

for ep in range(num_epochs):
  print ("Working on Epoch", ep+1)
  for x_batch, y_batch in dataloader_train:
    y_pred = model(x_batch)
    loss = loss_fun(y_pred, y_batch) # cross entropy between predictions and ground truth values
    model.zero_grad() # we need a clean slate before gradient calculation
    loss.backward() #back propagation
    optim.step() #update parameters